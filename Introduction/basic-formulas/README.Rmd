---
title: "Introduction | Basic Formulas"
author: "Craig A. Wendorf"
site: bookdown::bookdown_site
output: bookdown::bs4_book
url: https://cwendorf.github.io/Sourcebook
description: This page provides basic formulas for the calculations.
---

# Introduction | Basic Formulas

## Frequencies

### Primary Statistics

Percentiles: Percentiles provide the scores associated with particular percentile ranks. The 50<sup>th</sup> percentile (the Median) and the 25<sup>th</sup> and 75<sup>th</sup> percentiles (collectively known as the Interquartile Range) are the most commonly calculated.

> $$ \text{Position} = PR ( N + 1 ) $$

## Descriptives

### Central Tendency

Mean: The mean (or arithmetic average) is calculated as an unbiased estimate of the population mean. Here, the mean is determined as the average of the scores weighted by their frequencies:

> $$ M = \frac{\sum(fY)}{N} $$

### Variability

Sum of Squares: The Sum of Squares is the basic measure of the variability of the scores. Formally, it is the sum of the weighted deviations of the scores about the mean.

> $$ SS = \sum f (Y - M) $$

Mean Squares: Mean Squares (also known as Variance) is a function of the Sum of Squares. It is calculated as an unbiased estimate of the population variance.

> $$ MS = \frac{SS}{(N - 1)} $$

Standard Deviation: Standard Deviation is a function of the Mean Squares. It is also calculated as an unbiased estimate of the population standard deviation.

> $$ SD = \sqrt{MS} $$

## Correlations

### Preliminary Calculations

Sum of Cross Products: The Sum of Cross Products (SCP) is not easily determined solely from the summary statistics of the output, but rather from the data.

> $$ SCP = \sum ( X - M_X ) ( Y - M_Y ) $$

Covariance: The Covariance is a function of the Sum of Cross Products and the sample size:

> $$ COV = \frac{SCP}{(N - 1)} $$

### Primary Statistics

Pearson Correlation Coefficient: The Pearson Correlation Coefficient is a function of the Covariance and the Standard Deviations of both variables:

> $$ r = \frac{COV}{(SD_X) (SD_Y)} $$

## Standardized Scores

### Primary Statistics

Standardized (z) Score: A standardized score is a deviation score divided by the standard deviation. 

> $$ z = \frac{(Y - M)}{SD} $$

## Confidence Intervals

### Peliminary Calculations

Standard Error of the Mean: The standard error of the mean provides an estimate of how spread out the distribution of all possible random sample means would be.

> $$ SE_M = \frac{SD}{\sqrt{N}} $$

### Primary Statistics

Confidence Interval: For this test, the appropriate confidence interval is around (centered on) the mean difference (raw effect).

> $$ CI_M = M \pm (t_{CRITICAL}) (SE_M) $$

## One Sample t Test

### Peliminary Calculations

Mean Difference (Raw Effect): The Mean Difference is the difference between the sample mean and a user-specified test value or population mean.

> $$ M_{DIFF} = M - \mu $$

Standard Error of the Mean: The standard error of the mean provides an estimate of how spread out the distribution of all possible random sample means would be.

> $$ SE_M = \frac{SD}{\sqrt{N}} $$

### Primary Statistics

Statistical Significance: The *t* statistic is the ratio of the mean difference (raw effect) to the standard error of the mean.

> $$ t = \frac{M_{DIFF}}{SE_M} $$

Confidence Interval: For this design, the appropriate confidence interval is around (centered on) the mean difference (raw effect).

> $$ CI_{DIFF} = M_{DIFF} \pm (t_{CRITICAL} ) (SE_M) $$

Effect Size: Cohen’s *d* Statistic provides a standardized effect size for the mean difference (raw effect).

> $$ d = \frac{M_{DIFF}}{SD} $$

## Paired Samples t Test

### Peliminary Calculations

For the Paired Samples t test, the focus is on the change variable. As a result, it is the only variable that is used in the Introduction below.

Mean Difference (Raw Effect): The Mean Difference is the difference between the sample mean and a user-specified test value or population mean.

> $$ M_{DIFF} = M - \mu $$

### Primary Statistics

Statistical Significance: The *t* statistic is the ratio of the mean difference (raw effect) to the standard error of the mean.

> $$ t = \frac{M_{DIFF}}{SE_M} $$

Confidence Interval: For this test, the appropriate confidence interval is around (centered on) the mean difference (raw effect).

> $$ CI_{DIFF} = M_{DIFF} \pm (t_{CRITICAL} ) (SE_M) $$

Effect Size: Cohen’s *d* Statistic provides a standardized effect size for the mean difference (raw effect).

> $$ d = \frac{M_{DIFF}}{SD} $$

## Independent Samples t Test

### Peliminary Calculations

Mean Difference (Raw Effect): The mean difference is the difference between the two sample means (raw effect).

> $$ M_{DIFF} = M_1 - M_2 $$

Within Groups Statistics: When multiple groups are used, it is necessary to get an estimate of the pooled (combined) within group variabilities.

> $$ SS_1 = ( SD_1^2 ) ( df_1)  $$
>
> $$ SS_2 = ( SD_2^2 ) ( df_2) $$
>
> $$ SS_{WITHIN} = SS_1 + SS_ 2 $$
>
> $$ df_{WITHIN} = df_1 + df_ 2 $$
>
> $$ MS_{WITHIN} = \frac{SS_{WITHIN}}{df_{WITHIN}} $$
>
> $$ SD_{WITHIN} = \sqrt{MS_{WITHIN}} $$

Standard Error of the Difference: The standard error of the difference is a function of the two groups’ individual standard errors. 

> When the two sample sizes are equal:
>
> $$ SE_{DIFF} = \sqrt{ SE_1^2 + SE_2^2 } $$

> Or an expanded version of the formula can be used when the two sample sizes are either equal or unequal:
>
> $$ SE_{DIFF} = \sqrt{ \left( \frac{MS_{WITHIN}}{n_1} \right) + \left( \frac{MS_{WITHIN}}{n_2} \right) } $$

### Primary Statistics

Statistical Significance: The *t* statistic is the ratio of the mean difference (raw effect) to the standard error of the difference.

> $$ t = \frac{M_{DIFF}}{SE_{DIFF}} $$
>
> $$ df = ( n_1 - 1 ) + ( n_2 - 1) $$

Confidence Interval: For this test, the appropriate confidence interval is around (centered on) the mean difference (raw effect).

> $$ CI_{DIFF} = M_{DIFF} \pm (t_{CRITICAL}) ( SE_{DIFF}) $$

Effect Size: Cohen’s *d* Statistic provides a standardized effect size for the difference between the two means.

> $$ d = \frac{M_{DIFF}}{SD_{WITHIN}} $$

## One-Way ANOVA

### Peliminary Calculations

Grand (or Total) Mean: A grand mean can be determined by taking the weighted average of all of the group means.

> $$ M_{TOTAL} = \frac{\sum n_{GROUP} (M_{GROUP})}{N} $$

Between Groups Statistics: The between-groups effect statistics are a function of the group (level) means and sample sizes.

> $$ SS_{BETWEEN} = \sum n_{GROUP} (M_{GROUP} - M_{TOTAL})^2 $$
>
> $$ df_{BETWEEN} = \text{# groups} − 1 $$
>
> $$ MS_{BETWEEN} = \frac{SS_{BETWEEN}}{df_{BETWEEN}} $$

Within Groups Statistics: Within-groups error statistics are a function of the within group variabilities.

> $$ SS_1 = ( SD_1^2 ) ( df_1 ) $$
>
> $$ SS_2 = ( SD_2^2 ) ( df_2 ) $$
>
> $$ SS_3 = ( SD_3^2 ) ( df_3) $$
>
> $$ SS_{WITHIN} = SS_1 + SS_2 + SS_3 $$
>
> $$ df_{WITHIN} = df_1 + df_2 + df_3 $$
>
> $$ MS_{WITHIN} = \frac{SS_{WITHIN}}{df_{WITHIN}} $$

### Primary Statistics

Statistical Significance: The *F* statistic is the ratio of the between- and within-group variance estimates. 

> $$ F = \frac{MS_{BETWEEN}}{MS_{WITHIN}} $$

Confidence Intervals: For ANOVA, calculate the confidence intervals around (centered on) each mean separately.

> $$ CI_{M_1} = M_1 \pm (t_{CRITICAL}) (SE_{M_1}) $$

Effect Size: The Eta-Squared statistic is a ratio of the between group and the total group variability (Sum of Squares) estimates.

> $$ \eta^2 = \frac{SS_{BETWEEN}}{( SS_{BETWEEN} + SS_{WITHIN} )} $$

## Post Hoc Comparisons

### Peliminary Calculations

Mean Differences: Mean Differences (raw effects) are the differences between the means for all pairs of groups. Half of the possible pairwise comparisons are redundant and do not need to be calculated (though the mean differences will have the opposite signs because of subtraction order if they were calculated).

> $$ M_1 - M_2 $$

Standard Error of the Difference: These standard errors are for the difference between the two group means in each comparison. The values are a function of the MS<sub>WITHIN</sub> (from the ANOVA) and the sample sizes. \[In this case, because all groups are of the same size, the standard error for each comparison is the same.\]

> $$ SE_{DIFF} = \sqrt{ \left( \frac{MS_{WITHIN}}{n_{GROUP}} \right) + \left( \frac{MS_{WITHIN}}{n_{GROUP}} \right) } $$

### Primary Statistics

Statistical Significance: The *HSD* statistic is a ratio of the mean difference to the standard error of the difference. There is one statistic for each of the comparisons.

> $$ HSD_{1vs2} = \frac{( M_1 - M_2 )}{SE_{DIFF}} $$

Confidence Intervals: For *HSD*, calculate the confidence intervals around (centered on) each mean difference separately.

> $$ CI_{1vs2} = ( M_1 - M_2 ) \pm (HSD_{CRITICAL}) ( SE_{DIFF}) $$

## Repeated Measures ANOVA

### Peliminary Calculations

Grand (or Total) Mean: Because sample sizes are equal, a grand mean can be determined by averaging the level means.

> $$ M_{TOTAL} = \frac{ ( M_1 + M_2 ) }{2} $$

Subject Means: Each subject in the study would have an average score across the time points.

> $$ M_{SUBJECT1} = \frac{ ( Y_1 + Y_2 ) }{2} $$

Between-Subjects Error Statistics: Between-subjects error refers to the average differences across the participants of the study. This Sum of Squares is not easily determined from the summary statistics output, but rather from the data.

> $$ SS_{SUBJECTS} = \sum k (M_{SUBJECT} - M_{TOTAL})^2 $$
>
> $$ df_{SUBJECTS} = \text{# subjects} − 1 $$
>
> $$ MS_{SUBJECTS} = \frac{SS_{SUBJECTS}}{df_{SUBJECTS}} $$

Within-Subjects Error Statistics: The within-subjects error is a function of variabilities of the separate levels or conditions of the independent variable and the between-subjects error given above.

> $$ SS_1 = ( SD_1^2 ) ( df_1 ) $$
>
> $$ SS_2 = ( SD_2^2 ) ( df_2 ) $$
>
> $$ SS_{ERROR} = SS_1 + SS_2 - SS_{SUBJECTS} $$
>
> $$ df_{ERROR} = df_1 + df_2 - df_{SUBJECTS} $$
>
> $$ MS_{ERROR} = \frac{SS_{ERROR}}{df_{ERROR}} $$

Within-Subjects Effect Statistics: The statistics for the effect (or change) over time are functions of the means of the levels or conditions and the sample sizes.

> $$ SS_{EFFECT} = \sum n_{LEVEL} (M_{LEVEL} - M_{TOTAL})^2 $$
>
> $$ df_{EFFECT} = \text{# levels} − 1 $$
>
> $$ MS_{EFFECT} = \frac{SS_{EFFECT}}{df_{EFFECT}} $$

### Primary Statistics

Statistical Significance: The *F* statistic is the ratio of the within-subjects effect and the within-subjects error variance estimates. 

> $$ F = \frac{MS_{EFFECT}}{MS_{ERROR}} $$

Confidence Intervals: For RMD ANOVA, calculate the confidence intervals around (centered on) each mean separately.

> $$ CI_{M_1} = M_1 \pm (t_{CRITICAL}) (SE_{M_1}) $$

Effect Size: The partial eta-squared statistic is a ratio of the within-subjects effect and the remaining variability (Sum of Squares) estimates after between-subjects error has been partialled out.

> $$ \text{Partial} \; \eta^2 = \frac{SS_{EFFECT}}{( SS_{EFFECT} + SS_{ERROR} )} $$

## Factorial ANOVA

### Peliminary Calculations

Grand (or Total) Mean: A grand mean can be determined by taking the weighted average of all of the group means.

> $$ M_{TOTAL} = \frac{\sum n_{GROUP} (M_{GROUP})}{N} $$

Marginal Means: A level (marginal) mean can be determined by taking the weighted average of the appropriate group means.

> For Factor A:  
>
> $$ M_{A1} = \frac{\sum n_{GROUP} (M_{GROUP})}{N_{LEVEL}} $$
>
> $$ M_{A2} = \frac{\sum n_{GROUP} (M_{GROUP})}{N_{LEVEL}} $$

> For Factor B: 
> 
> $$ M_{B1} = \frac{\sum n_{GROUP} (M_{GROUP})}{N_{LEVEL}} $$
>
> $$ M_{B2} = \frac{\sum n_{GROUP} (M_{GROUP})}{N_{LEVEL}} $$

Error (Within Groups) Statistics: Within-groups error statistics are a function of the within group variabilities.

> $$ SS_1 = ( SD_1^2 ) ( df_1 ) $$
>
> $$ SS_2 = ( SD_2^2 ) ( df_2 ) $$
>
> $$ SS_3 = ( SD_3^2 ) ( df_3 ) $$
>
> $$ SS_4 = ( SD_4^2 ) ( df_4 ) $$
>
> $$ SS_{ERROR} = SS_1 + SS_2 + SS_3 + SS_4 $$
>
> $$ df_{ERROR} = df_1 + df_2 + df_3 +df_4 $$
>
> $$ MS_{ERROR} = \frac{SS_{ERROR}}{df_{ERROR}} $$

Effect (Between Groups) Statistics: The Model statistics represent the overall differences among the groups. The Factor A and Factor B statistics are a function of the level (marginal) means and sample sizes. The interaction statistics reflect the between-groups variability not accounted for by the factors individually.

> For the Model:  
>
> $$ SS_{MODEL} = \sum n_{GROUP} (M_{GROUP} - M_{TOTAL})^2 $$
>
> $$ df_{MODEL} = \text{# groups} − 1 $$

> For Factor A:  
>
> $$ SS_{FACTORA} = \sum n_{LEVEL} (M_{LEVEL} - M_{TOTAL})^2 $$
>
> $$ df_{FACTORA} = \text{# levels} − 1 $$
>
> $$ MS_{FACTORA} = \frac{SS_{FACTORA}}{df_{FACTORA}} $$

> For Factor B:  
>
> $$ SS_{FACTORB} = \sum n_{LEVEL} (M_{LEVEL} - M_{TOTAL})^2 $$
>
> $$ df_{FACTORB} = \text{# levels} − 1 $$
>
> $$ MS_{FACTORB} = \frac{SS_{FACTORB}}{df_{FACTORB}} $$

> For the Interaction:  
>
> $$ SS_{INTER} = SS_{MODEL} - SS_{FACTORA} - SS_{FACTORB} $$
>
> $$ df_{INTER} = df_{MODEL} - df_{FACTORA} - df_{FACTORB} $$
>
> $$ MS_{INTER} = \frac{SS_{INTER}}{df_{INTER}} $$

### Primary Statistics

Statistical Significance: The *F* statistic is the ratio of the between-and within-group variance estimates. 

> For the Factor A Main Effect:  
>
> $$ F = \frac{MS_{FACTORA}}{MS_{ERROR}} $$

> For the Factor B Main Effect:  
>
> $$ F = \frac{MS_{FACTORB}}{MS_{ERROR}} $$
>
> For the Interaction:  
>
> $$ F = \frac{MS_{INTER}}{MS_{ERROR}} $$

Effect Size: The partial eta-squared statistic is a ratio of the between-subjects effect and the remaining variability (Sum of Squares) estimates after within-subjects error has been partialled out.

> For the Factor A Main Effect:  
>
> $$ \text{Partial} \; \eta^2 = \frac{SS_{FACTORA}}{( SS_{FACTORA} + SS_{ERROR} )} $$

> For the Factor B Main Effect:  
>
> $$ \text{Partial} \; \eta^2 = \frac{SS_{FACTORB}}{( SS_{FACTORB} + SS_{ERROR} )} $$

> For the Interaction:  
>
> $$ \text{Partial} \; \eta^2 = \frac{SS_{INTER}}{( SS_{INTER} + SS_{ERROR} )} $$

Confidence Intervals: For Factorial ANOVA, calculate the confidence intervals around (centered on) each mean separately (not shown here).
